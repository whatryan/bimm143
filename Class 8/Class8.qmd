---
title: "Class 08: Machine Learning Mini Project"
Author: "Ryan Chung A15848050"
format: html
---
# Breast Cancer Project
Today we are going to explore some data from the University of Wisconsin Cancer Center on Breast Biopsy data

```{r}

wisc.df <- read.csv('WisconsinCancer.csv', row.names = 1)

head(wisc.df)

#diagnosis - M = Malignant, B = benign 

```

>Q1. How many patient samples are in this dataset

```{r}

nrow(wisc.df)
table(wisc.df$diagnosis)

```
 
 There are `r nrow(wisc.df)` patients in this dataset.

>Q2. How many cancer (M) and non cancer (B) samples are there?

There are `r table(wisc.df$diagnosis)` non cancer(B) and cancer(M) samples respectively. 

Save the diagnosis for later use as a reference to compare how well we do with PCA etc.

```{r}

diagnosis <- as.factor(wisc.df$diagnosis) # use this for categorical data is ez
#diagnosis

```



Now exclude the diagnosis column from the data

```{r}

wisc <- wisc.df[ , -1] #remove the diagnosis, put it in a new df
ncol(wisc)

```


>Q3. How many variables/features/dimensions are in the data w/o the diagnosis?

There are `r ncol(wisc)` features in the data.

#Principal Component Analysis

To perform PCA in R we can use the `prcomp()` function it takes as input a numeric dataset and optional `scale = FALSE/TRUE` argument

PCA aims to take a dataset with lots of dimensions and flattens it to 2 or 3 dimensions 

Scaling is an issue, the largest number factor will dominate the PCA without scaling

We generally always want to set the `scale = TRUE` but let's make sure by checking if the mean and stdev values are different across these 30 columns

```{r}

round( colMeans(wisc))

```

```{r}

pca <- prcomp(wisc, scale = T)
summary(pca)
#these are ordered by importance (proportion)
```

>Q4: What proportion of the original variance is captured by the first princiapl components (PC1)?

Approximately 44% is captured by PC1.

>Q5: How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are needed to get to 73% of the variance.

>Q6 How many principal components (PCs) are requred to describe at least 90% of the original variance in the data?

7 principal components are required to describe at least 90% of the original variance.

>Q7 What stands out to you about this plot? Is it easy or difficult to understand and why?

It is extremely difficult to understand as the data points and labels are overlapping and it is illegible. 
```{r}
biplot(pca)
```


```{r}
attributes(pca)
```

```{r}

plot(pca$x[ ,1], pca$x[ ,2], col = diagnosis) #gives where these patients lie on the axes

```

```{r}
#same plot as above but in ggplot
x<-as.data.frame(pca$x)
library(ggplot2)
ggplot(x) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()

```

>Q8: Generate a similar plot for principal components 1 and 3. What do you notice?
  
  These plots are very similar but the second plot seems to be more centered around the 0 point of the PC axes.

```{r}
plot(pca$x[ ,1], pca$x[ ,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```


>Q. How much variance is captured in the top 3 PCs?

72.6% of the variance is captured in the top PCs.

```{r}
summary(pca)
 #cumulative proportion at PC3 is .72636 = 72.6%


```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

This tells us how much this original feature contributes to the first PC

>Q10: What is the minimum number of principal components required to explain 80% of the variance of the data?

You need a minimum of 5 principal components to explain at least 80% of the variance in the data.

The weight is `rpca$rotation["concave.points_mean", 1]` for `concave.points_mean`

```{r}

pca$rotation
  #This is the weight of each variable in each PC

pca$rotation["concave.points_mean", 1]

```


#Combining Methods

We can use our new PCA variables(i.e. the scores along the PCs contained in t `pca$x`) as input for other methods such as clustering

```{r}

#prcomp(data, scale = true)
d <- dist(pca$x[ , 1:3])

#hclust(dist(data), method =) hclust needs a distance matrix as input
hc <- hclust(d, method = 'ward.D2')
plot(hc)
abline(h = 39, col = 'red', lty = 2)
```

>Q11: What is the height at which the clustering model has 4 clusters?

  The clustering model above has 4 clusters at about a height at h = 39.

To get our cluster membership vector we can use the `cutree()` function and specify a height (`h`) or the number of groups(`k`)

>Q12: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?



Yes the higher the number of clusters the more distinct the cluster identity is at the tail ends of the groups. However, there will always be a spread and overlapping diagnoses within the center groups.


>Q13: Which methods gives your favorite results for the same data.dist dataset?
 
I like to use `ward.d2` because I was able to grasp how that works during lecture and it helps me to understand what is happening overall with the data.

```{r}

#cutree(hc, h = 80) #Cut the tree at height of 80
cutree(hc, k = 2) #Cut the tree to give me 2 groups

#table(cutree(hc,h = 80))
grps <-(cutree(hc,k = 5 ))
table(grps)
```

I want to find out how many diagnosis "M" and "B" are in each group. How?

```{r}

table(diagnosis)

table(diagnosis, grps) #tells u how many b and m are in groups 1 and 2 in a confusing way 
  #in grp 1 there were 179 malignant and 24 benign samples
  #in grp 2 there were  33 malignant and 333 benign samples
```

We can also plot our results using our clustering vector `grps`

```{r}
plot(pca$x[ ,1], pca$x[ ,2], col = grps) #gives where these patients lie on the axes


x<-as.data.frame(pca$x)
library(ggplot2)
ggplot(x) +
  aes(PC1, PC2) +
  geom_point(col = grps)

#the overlap in both plots comes from the fact that we clustered by PC 1:3 but only plotted PC1:2

```

>Q15/ What is the specificity and sensitivity of our current results

Sensitivity - do u capture all 'positives'

Specificity - how well do u distinguish healthy from positive patients

```{r}

table(diagnosis, grps)

sensitivity = 179/(179 + 33) # TP/(TP+FN) TP = True Pos, FN =                                                             False neg
specificity = 333/(333 + 33) # TN/(TN+FN) TN = True Neg, 
```

#Prediction

```{r}

url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(pca, newdata=new)
npc

```

